{"cells":[{"cell_type":"markdown","source":["# Tools for NLP\n\nThere are lots of feature transformations that need to be done on text data to get it to a point that machine learning algorithms can understand. Luckily, Spark has placed the most important ones in convienent Feature Transformer calls. \n\nLet's go over them before jumping into the project."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07703610-7074-40be-b797-39281d17d76d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e791403e-eb3a-40ce-9d75-0cd9b42a80b6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('nlp').getOrCreate()"],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"90dd7519-379b-4b39-86a6-3a6422b284e0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Tokenizer\n<p><a href=\"http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\">Tokenization</a> is the process of taking text (such as a sentence) and breaking it into individual terms (usually words).  A simple <a href=\"api/scala/index.html#org.apache.spark.ml.feature.Tokenizer\">Tokenizer</a> class provides this functionality.  The example below shows how to split sentences into sequences of words.</p>\n\n<p><a href=\"api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer\">RegexTokenizer</a> allows more\n advanced tokenization based on regular expression (regex) matching.\n By default, the parameter &#8220;pattern&#8221; (regex, default: <code>\"\\\\s+\"</code>) is used as delimiters to split the input text.\n Alternatively, users can set parameter &#8220;gaps&#8221; to false indicating the regex &#8220;pattern&#8221; denotes\n &#8220;tokens&#8221; rather than splitting gaps, and find all matching occurrences as the tokenization result.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da904f86-225f-4985-9af3-16f0d3fbde95","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType"],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6bc4b4e9-1ebe-4a08-bc2f-c45016c5300e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat\")\n], [\"id\", \"sentence\"])"],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"94d62012-66ed-4a6a-a96c-36b95b556f47","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sentenceDataFrame.show()"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"669520be-7da7-44ef-ab1f-9d14f57ea392","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------------+\n| id|            sentence|\n+---+--------------------+\n|  0|Hi I heard about ...|\n|  1|I wish Java could...|\n|  2|Logistic,regressi...|\n+---+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n# Since the split is based on spaces, not commas, there is only one token.\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"010cefbe-d831-4761-af93-b6234a87c909","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Stop Words Removal\n\n<p><a href=\"https://en.wikipedia.org/wiki/Stop_words\">Stop words</a> are words which\nshould be excluded from the input, typically because the words appear\nfrequently and don&#8217;t carry as much meaning.</p>\n\n<p><code>StopWordsRemover</code> takes as input a sequence of strings (e.g. the output\nof a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>) and drops all the stop\nwords from the input sequences. The list of stopwords is specified by\nthe <code>stopWords</code> parameter. Default stop words for some languages are accessible \nby calling <code>StopWordsRemover.loadDefaultStopWords(language)</code>, for which available \noptions are &#8220;danish&#8221;, &#8220;dutch&#8221;, &#8220;english&#8221;, &#8220;finnish&#8221;, &#8220;french&#8221;, &#8220;german&#8221;, &#8220;hungarian&#8221;, \n&#8220;italian&#8221;, &#8220;norwegian&#8221;, &#8220;portuguese&#8221;, &#8220;russian&#8221;, &#8220;spanish&#8221;, &#8220;swedish&#8221; and &#8220;turkish&#8221;. \nA boolean parameter <code>caseSensitive</code> indicates if the matches should be case sensitive \n(false by default).</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1944d2ee-073a-4fa8-b50f-44a803c10d57","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)\n"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"653f170f-50b5-4d1b-bc84-33ba8600baf7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## n-grams\n\nAn n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n\n<p><code>NGram</code> takes as input a sequence of strings (e.g. the output of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>).  The parameter <code>n</code> is used to determine the number of terms in each $n$-gram. The output will consist of a sequence of $n$-grams where each $n$-gram is represented by a space-delimited string of $n$ consecutive words.  If the input sequence contains fewer than <code>n</code> strings, no output is produced.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"567a06db-23ec-4825-a31b-a22c30820b99","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"236d3862-22ea-40a3-8fc9-3b9aa2b6c971","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["_______\n# Feature Extractors\n_______"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afcec0b3-d7af-4b63-804b-346a53a3d11b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<h2 id=\"tf-idf\">TF-IDF</h2>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Term frequency-inverse document frequency (TF-IDF)</a> \nis a feature vectorization method widely used in text mining to reflect the importance of a term \nto a document in the corpus. Denote a term by <code>$t$</code>, a document by  d , and the corpus by D.\nTerm frequency <code>$TF(t, d)$</code> is the number of times that term <code>$t$</code> appears in document <code>$d$</code>, while \ndocument frequency <code>$DF(t, D)$</code> is the number of documents that contains term <code>$t$</code>. If we only use \nterm frequency to measure the importance, it is very easy to over-emphasize terms that appear very \noften but carry little information about the document, e.g. &#8220;a&#8221;, &#8220;the&#8221;, and &#8220;of&#8221;. If a term appears \nvery often across the corpus, it means it doesn&#8217;t carry special information about a particular document.\nInverse document frequency is a numerical measure of how much information a term provides:\n\n$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n\nwhere |D| is the total number of documents in the corpus. Since logarithm is used, if a term \nappears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \ndividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b36894f-672f-40c8-8012-0c9ddb17730e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\nsentenceData.show()"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf6cd956-17fb-4aeb-8807-aafcea778bb7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------------+\n|label|            sentence|\n+-----+--------------------+\n|  0.0|Hi I heard about ...|\n|  0.0|I wish Java could...|\n|  1.0|Logistic regressi...|\n+-----+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\nwordsData.show()"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"287b8b3c-9b27-4fd5-9709-f6f1de376bab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+\n|label|            sentence|               words|\n+-----+--------------------+--------------------+\n|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n|  0.0|I wish Java could...|[i, wish, java, c...|\n|  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ddb5c6fd-87f6-42fc-9342-4c2351389c60","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[6,8,13,16],[...|\n|  0.0|(20,[0,2,7,13,15,...|\n|  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## CountVectorizer\nCountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n\nDuring the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f2b4861-15da-4ad8-8537-2167d4fe66fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ab98face-863e-41a4-90bf-e7f440ece4aa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"]}],"execution_count":0}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Tools_for_NLP","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
